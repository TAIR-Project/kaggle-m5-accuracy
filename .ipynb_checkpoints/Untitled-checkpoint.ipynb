{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import seaborn as sns\n",
    "from optuna.integration import lightgbm_tuner\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minih\\python_prac1\\m5\\git-repository\\kaggle-m5-accuracy-library\\library\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\minih\\python_prac1\\m5\\git-repository\\kaggle-m5-accuracy-library\\library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patair import reduce_mem_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minih\\python_prac1\\library\\m5\n"
     ]
    }
   ],
   "source": [
    "cd C:/Users/minih/python_prac1/library/m5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  0.64 Mb (0.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "product = pd.read_pickle(\"product.pickle\").pipe(reduce_mem_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = pd.read_pickle(\"all_sales_train_validation_translated_contain_null.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_feature=[\n",
    "               'prev_28', 'p_prev_1','rolling_mean_t7', \n",
    "               'rolling_mean_t14', 'price_change_t1','rolling_std_t7', 'rolling_std_t14',\n",
    "               'rolling_price_std_t7', 'rolling_price_std_t30', 'price_change_t1', 'rolling_price_std_t7',\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataの時系列特徴列(prev_〇など)をみてnanを含むデータ削除する\n",
    "#len(np.unique(datas.dropna(subset=check_feature)[\"id\"]))\n",
    "datas.dropna(subset=check_feature, inplace=True)\n",
    "#len(np.unique(datas.dropna(subset=check_feature)[\"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                               object\n",
       "item_id                           int16\n",
       "dept_id                            int8\n",
       "cat_id                             int8\n",
       "store_id                           int8\n",
       "state_id                           int8\n",
       "sell_num                        float16\n",
       "date                     datetime64[ns]\n",
       "event_name_1                       int8\n",
       "event_type_1                       int8\n",
       "event_name_2                       int8\n",
       "event_type_2                       int8\n",
       "snap_CA                            int8\n",
       "snap_TX                            int8\n",
       "snap_WI                            int8\n",
       "sell_price                      float16\n",
       "prev_28                         float16\n",
       "p_prev_1                        float16\n",
       "rolling_mean_t7                 float16\n",
       "rolling_mean_t14                float16\n",
       "rolling_mean_t30                float16\n",
       "rolling_std_t7                  float16\n",
       "rolling_std_t14                 float16\n",
       "rolling_std_t30                 float16\n",
       "price_change_t1                 float16\n",
       "rolling_price_std_t7            float16\n",
       "rolling_price_std_t30           float16\n",
       "year                              int16\n",
       "quarter                            int8\n",
       "month                              int8\n",
       "week                               int8\n",
       "day                                int8\n",
       "dayofweek                          int8\n",
       "is_weekend                         int8\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRMSSEの計算用(kernelより)\n",
    "\n",
    "NUM_ITEMS = 30490\n",
    "DAYS_PRED = 28\n",
    "\n",
    "weight_mat = np.c_[np.ones([NUM_ITEMS,1]).astype(np.int8), # level 1\n",
    "                   pd.get_dummies(product.state_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.store_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.state_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.state_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.store_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.store_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.item_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   pd.get_dummies(product.state_id.astype(str) + product.item_id.astype(str),drop_first=False).astype('int8').values,\n",
    "                   np.identity(NUM_ITEMS).astype(np.int8) #item :level 12\n",
    "                   ].T\n",
    "\n",
    "weight_mat_csr = csr_matrix(weight_mat)\n",
    "del weight_mat; gc.collect()\n",
    "\n",
    "def weight_calc(data,product):\n",
    "    \n",
    "    # calculate the denominator of RMSSE, and calculate the weight base on sales amount\n",
    "\n",
    "    sales_train_val = pd.read_csv('sales_train_validation.csv')\n",
    "\n",
    "    d_name = ['d_' + str(i+1) for i in range(1913)]\n",
    "\n",
    "    sales_train_val = weight_mat_csr * sales_train_val[d_name].values\n",
    "\n",
    "    # calculate the start position(first non-zero demand observed date) for each item / 商品の最初の売上日\n",
    "    # 1-1914のdayの数列のうち, 売上が存在しない日を一旦0にし、0を9999に置換。そのうえでminimum numberを計算\n",
    "    df_tmp = ((sales_train_val>0) * np.tile(np.arange(1,1914),(weight_mat_csr.shape[0],1)))\n",
    "\n",
    "    start_no = np.min(np.where(df_tmp==0,9999,df_tmp),axis=1)-1\n",
    "\n",
    "    flag = np.dot(np.diag(1/(start_no+1)) , np.tile(np.arange(1,1914),(weight_mat_csr.shape[0],1)))<1\n",
    "\n",
    "    sales_train_val = np.where(flag,np.nan,sales_train_val)\n",
    "\n",
    "    # denominator of RMSSE / RMSSEの分母\n",
    "    weight1 = np.nansum(np.diff(sales_train_val,axis=1)**2,axis=1)/(1913-start_no)\n",
    "\n",
    "    # calculate the sales amount for each item/level\n",
    "    df_tmp = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n",
    "    df_tmp['sell_num'] = df_tmp['sell_num'] * df_tmp['sell_price']\n",
    "    df_tmp =df_tmp.groupby(['id'])['sell_num'].apply(np.sum)\n",
    "    df_tmp = df_tmp[product.id].values\n",
    "    \n",
    "    weight2 = weight_mat_csr * df_tmp \n",
    "\n",
    "    weight2 = weight2/np.sum(weight2)\n",
    "\n",
    "    del sales_train_val\n",
    "    gc.collect()\n",
    "    \n",
    "    return weight1, weight2\n",
    "\n",
    "#weight1, weight2 = weight_calc(datas,product)\n",
    "\n",
    "def wrmsse(preds, data):\n",
    "    \n",
    "    # this function is calculate for last 28 days to consider the non-zero demand period\n",
    "    \n",
    "    # actual obserbed values / 正解ラベル\n",
    "    y_true = data.get_label()\n",
    "    \n",
    "    y_true = y_true[-(NUM_ITEMS * DAYS_PRED):]\n",
    "    preds = preds[-(NUM_ITEMS * DAYS_PRED):]\n",
    "    # number of columns\n",
    "    num_col = DAYS_PRED\n",
    "    \n",
    "    # reshape data to original array((NUM_ITEMS*num_col,1)->(NUM_ITEMS, num_col) ) / 推論の結果が 1 次元の配列になっているので直す\n",
    "    #print(preds)\n",
    "    #print(y_true)\n",
    "    reshaped_preds = preds.reshape(num_col, NUM_ITEMS).T\n",
    "    reshaped_true = np.array(y_true).reshape(num_col, NUM_ITEMS).T\n",
    "    \n",
    "          \n",
    "    train = weight_mat_csr*np.c_[reshaped_preds, reshaped_true]\n",
    "    #print(train)\n",
    "    \n",
    "    score = np.sum(\n",
    "                np.sqrt(\n",
    "                    np.mean(\n",
    "                        np.square(\n",
    "                            train[:,:num_col] - train[:,num_col:])\n",
    "                        ,axis=1) / weight1) * weight2)\n",
    "    \n",
    "    return 'wrmsse', score, False\n",
    "\n",
    "def wrmsse_simple(preds, data):\n",
    "    \n",
    "    # actual obserbed values / 正解ラベル\n",
    "    y_true = data.get_label()\n",
    "    \n",
    "    y_true = y_true[-(NUM_ITEMS * DAYS_PRED):]\n",
    "    preds = preds[-(NUM_ITEMS * DAYS_PRED):]\n",
    "    # number of columns\n",
    "    num_col = DAYS_PRED\n",
    "    \n",
    "    # reshape data to original array((NUM_ITEMS*num_col,1)->(NUM_ITEMS, num_col) ) / 推論の結果が 1 次元の配列になっているので直す\n",
    "    reshaped_preds = preds.reshape(num_col, NUM_ITEMS).T\n",
    "    reshaped_true = y_true.reshape(num_col, NUM_ITEMS).T\n",
    "          \n",
    "    train = np.c_[reshaped_preds, reshaped_true]\n",
    "   \n",
    "    \n",
    "    weight2_2 = weight2[:NUM_ITEMS]\n",
    "    weight2_2 = weight2_2/np.sum(weight2_2)\n",
    "    \n",
    "    score = np.sum(\n",
    "                np.sqrt(\n",
    "                    np.mean(\n",
    "                        np.square(\n",
    "                            train[:,:num_col] - train[:,num_col:])\n",
    "                        ,axis=1) /  weight1[:NUM_ITEMS])*weight2_2)\n",
    "    \n",
    "    return 'wrmsse', score, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight_mat_csr\n",
    "weights = pd.read_pickle(\"weight.pickle\")\n",
    "weight1 = weights[weights.index ==0]\n",
    "weight2 = weights[weights.index ==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight1=weight1.values\n",
    "weight2=weight2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x_train = datas[datas['date'] < '2016-04-18']\n",
    "x_train = datas[datas['date'] < '2016-03-28']\n",
    "y_train = x_train['sell_num']\n",
    "#x_val = datas[datas['date'] >= '2016-04-18']\n",
    "x_val = datas[(datas['date'] >= '2016-03-28')& (datas['date'] <= '2016-04-24')]\n",
    "y_val = x_val['sell_num']\n",
    "test = datas[datas['date'] > '2016-04-24']\n",
    "del datas\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "   #訓練に用いる特徴量をここに記述\n",
    "    \"item_id\",\n",
    "    \"dept_id\",\n",
    "    \"cat_id\",\n",
    "    \"store_id\",\n",
    "    \"state_id\",\n",
    "    \"event_name_1\",\n",
    "    #\"event_type_1\",\n",
    "    \"event_name_2\",\n",
    "    #\"event_type_2\",\n",
    "    \n",
    "    \n",
    "    \"snap_CA\",\n",
    "    \"snap_TX\",\n",
    "    \"snap_WI\",\n",
    "    \n",
    "    \"sell_price\",\n",
    "    # demand features.\n",
    "    #'prev_1',\n",
    "    #'prev_2',\n",
    "    #'prev_7', \n",
    "    #'prev_14', \n",
    "    'prev_28',\n",
    "    #'prev_182', \n",
    "    #'prev_365', \n",
    "    'p_prev_1', \n",
    "    'rolling_mean_t7',\n",
    "    'rolling_mean_t14', \n",
    "    'rolling_mean_t30',\n",
    "    'rolling_std_t7',\n",
    "    'rolling_std_t14',\n",
    "    'rolling_std_t30',\n",
    "    #'rolling_mean_t60', \n",
    "    #'rolling_mean_t90',\n",
    "    #'rolling_mean_t180',  \n",
    "    # price features\n",
    "    \"price_change_t1\",\n",
    "    #\"price_change_t365\",\n",
    "    #'rolling_price_max_t365', \n",
    "    #'price_change_t365',\n",
    "    \"rolling_price_std_t7\",\n",
    "    \"rolling_price_std_t30\",\n",
    "    # time features.\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"week\",\n",
    "    \"day\",\n",
    "    \"dayofweek\",\n",
    "    #\"is_year_end\",\n",
    "    #\"is_year_start\",\n",
    "    #\"is_quarter_end\",\n",
    "    #\"is_quarter_start\",\n",
    "    #\"is_month_end\",\n",
    "    #\"is_month_start\",\n",
    "    #\"is_weekend\",\n",
    "]\n",
    "\n",
    "train_set = lgb.Dataset(x_train[features], y_train)\n",
    "val_set = lgb.Dataset(x_val[features], y_val)\n",
    "\n",
    "del x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用基本パラメータ\n",
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    \"objective\" : \"poisson\",\n",
    "    'metric': 'custom',\n",
    "    'rondom_state':32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minih\\anaconda3\\lib\\site-packages\\optuna\\_experimental.py:90: ExperimentalWarning: train is experimental (supported from v0.18.0). The interface can change in the future.\n",
      "  ExperimentalWarning,\n",
      "feature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py:1295: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['cat_id', 'dept_id', 'event_name_1', 'event_name_2', 'item_id', 'state_id', 'store_id']\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n",
      "[W 2020-05-29 16:10:37,659] Setting status of trial#0 as TrialState.FAIL because of the following error: ValueError('For early stopping, at least one dataset and eval metric is required for evaluation')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\optuna\\study.py\", line 699, in _run_trial\n",
      "    result = func(trial)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\optuna\\integration\\lightgbm_tuner\\optimize.py\", line 231, in __call__\n",
      "    booster = lgb.train(self.lgbm_params, self.train_set, **self.lgbm_kwargs)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\", line 264, in train\n",
      "    evaluation_result_list=evaluation_result_list))\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\callback.py\", line 221, in _callback\n",
      "    _init(env)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\callback.py\", line 191, in _init\n",
      "    raise ValueError('For early stopping, '\n",
      "ValueError: For early stopping, at least one dataset and eval metric is required for evaluation\n",
      "feature_fraction, val_score: inf:   0%|          | 0/7 [00:39<?, ?it/s]\n",
      "num_leaves, val_score: inf:   0%|          | 0/20 [00:00<?, ?it/s][W 2020-05-29 16:10:41,297] Setting status of trial#1 as TrialState.FAIL because of the following error: ValueError('For early stopping, at least one dataset and eval metric is required for evaluation')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\optuna\\study.py\", line 699, in _run_trial\n",
      "    result = func(trial)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\optuna\\integration\\lightgbm_tuner\\optimize.py\", line 231, in __call__\n",
      "    booster = lgb.train(self.lgbm_params, self.train_set, **self.lgbm_kwargs)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\", line 264, in train\n",
      "    evaluation_result_list=evaluation_result_list))\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\callback.py\", line 221, in _callback\n",
      "    _init(env)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\callback.py\", line 191, in _init\n",
      "    raise ValueError('For early stopping, '\n",
      "ValueError: For early stopping, at least one dataset and eval metric is required for evaluation\n",
      "num_leaves, val_score: inf:   0%|          | 0/20 [00:02<?, ?it/s]\n",
      "bagging, val_score: inf:   0%|          | 0/10 [00:00<?, ?it/s][W 2020-05-29 16:10:45,700] Setting status of trial#2 as TrialState.FAIL because of the following error: ValueError('For early stopping, at least one dataset and eval metric is required for evaluation')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\optuna\\study.py\", line 699, in _run_trial\n",
      "    result = func(trial)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\optuna\\integration\\lightgbm_tuner\\optimize.py\", line 231, in __call__\n",
      "    booster = lgb.train(self.lgbm_params, self.train_set, **self.lgbm_kwargs)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\", line 264, in train\n",
      "    evaluation_result_list=evaluation_result_list))\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\callback.py\", line 221, in _callback\n",
      "    _init(env)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\callback.py\", line 191, in _init\n",
      "    raise ValueError('For early stopping, '\n",
      "ValueError: For early stopping, at least one dataset and eval metric is required for evaluation\n",
      "bagging, val_score: inf:   0%|          | 0/10 [00:04<?, ?it/s]\n",
      "feature_fraction_stage2, val_score: inf:   0%|          | 0/6 [00:00<?, ?it/s][W 2020-05-29 16:10:50,353] Setting status of trial#3 as TrialState.FAIL because of the following error: ValueError('For early stopping, at least one dataset and eval metric is required for evaluation')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\optuna\\study.py\", line 699, in _run_trial\n",
      "    result = func(trial)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\optuna\\integration\\lightgbm_tuner\\optimize.py\", line 231, in __call__\n",
      "    booster = lgb.train(self.lgbm_params, self.train_set, **self.lgbm_kwargs)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\", line 264, in train\n",
      "    evaluation_result_list=evaluation_result_list))\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\callback.py\", line 221, in _callback\n",
      "    _init(env)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\callback.py\", line 191, in _init\n",
      "    raise ValueError('For early stopping, '\n",
      "ValueError: For early stopping, at least one dataset and eval metric is required for evaluation\n",
      "feature_fraction_stage2, val_score: inf:   0%|          | 0/6 [00:04<?, ?it/s]\n",
      "regularization_factors, val_score: inf:   0%|          | 0/20 [00:00<?, ?it/s][W 2020-05-29 16:10:54,553] Setting status of trial#4 as TrialState.FAIL because of the following error: ValueError('For early stopping, at least one dataset and eval metric is required for evaluation')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\optuna\\study.py\", line 699, in _run_trial\n",
      "    result = func(trial)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\optuna\\integration\\lightgbm_tuner\\optimize.py\", line 231, in __call__\n",
      "    booster = lgb.train(self.lgbm_params, self.train_set, **self.lgbm_kwargs)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\", line 264, in train\n",
      "    evaluation_result_list=evaluation_result_list))\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\callback.py\", line 221, in _callback\n",
      "    _init(env)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\callback.py\", line 191, in _init\n",
      "    raise ValueError('For early stopping, '\n",
      "ValueError: For early stopping, at least one dataset and eval metric is required for evaluation\n",
      "regularization_factors, val_score: inf:   0%|          | 0/20 [00:04<?, ?it/s]\n",
      "min_data_in_leaf, val_score: inf:   0%|          | 0/5 [00:00<?, ?it/s][W 2020-05-29 16:10:59,136] Setting status of trial#5 as TrialState.FAIL because of the following error: ValueError('For early stopping, at least one dataset and eval metric is required for evaluation')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\optuna\\study.py\", line 699, in _run_trial\n",
      "    result = func(trial)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\optuna\\integration\\lightgbm_tuner\\optimize.py\", line 231, in __call__\n",
      "    booster = lgb.train(self.lgbm_params, self.train_set, **self.lgbm_kwargs)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\", line 264, in train\n",
      "    evaluation_result_list=evaluation_result_list))\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\callback.py\", line 221, in _callback\n",
      "    _init(env)\n",
      "  File \"C:\\Users\\minih\\anaconda3\\lib\\site-packages\\lightgbm\\callback.py\", line 191, in _init\n",
      "    raise ValueError('For early stopping, '\n",
      "ValueError: For early stopping, at least one dataset and eval metric is required for evaluation\n",
      "min_data_in_leaf, val_score: inf:   0%|          | 0/5 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The best booster cannot be found. It may be found in the other processes due to resuming or distributed computing. Please set the `model_dir` argument of `LightGBMTuner.__init__` and make sure that boosters are shared with all processes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-7a92b8884b23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                     \u001b[0mcategorical_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"item_id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"dept_id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"cat_id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"store_id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"state_id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"event_name_1\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"event_name_2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                     \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                                     \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m                                     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\_experimental.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 )\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\integration\\lightgbm_tuner\\__init__.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mauto_booster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLightGBMTuner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mauto_booster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mauto_booster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_best_booster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\integration\\lightgbm_tuner\\optimize.py\u001b[0m in \u001b[0;36mget_best_booster\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model_dir\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m             raise ValueError(\n\u001b[1;32m--> 484\u001b[1;33m                 \u001b[1;34m\"The best booster cannot be found. It may be found in the other processes due to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m                 \u001b[1;34m\"resuming or distributed computing. Please set the `model_dir` argument of \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[1;34m\"`LightGBMTuner.__init__` and make sure that boosters are shared with all \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The best booster cannot be found. It may be found in the other processes due to resuming or distributed computing. Please set the `model_dir` argument of `LightGBMTuner.__init__` and make sure that boosters are shared with all processes."
     ]
    }
   ],
   "source": [
    " # Optuna でハイパーパラメータを Stepwise Optimization する\n",
    "tuned_booster = lightgbm_tuner.train(lgb_params, train_set,\n",
    "                                     valid_sets = [train_set, val_set],\n",
    "                                     num_boost_round=1000,\n",
    "                                     categorical_feature = [\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\",\"event_name_1\",\"event_name_2\"],\n",
    "                                     early_stopping_rounds=50,\n",
    "                                     verbose_eval=20,\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
